{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import islice, combinations\n",
    "from functools import reduce\n",
    "from multiprocessing import Pool\n",
    "from scipy.linalg import qr\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.cluster.hierarchy import fcluster, leaves_list, optimal_leaf_ordering\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fastcluster import linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-contributor",
   "metadata": {},
   "source": [
    "# Signal preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-ministry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_membership(xs_len, mean_prop):\n",
    "    dates = pd.bdate_range('2010', '2020')\n",
    "    membership = np.random.rand(len(dates), xs_len) < mean_prop\n",
    "    return pd.DataFrame(index=dates, data=membership)\n",
    "\n",
    "def gen_data(membership):\n",
    "    data = np.random.randn(*membership.shape)\n",
    "    return (\n",
    "        pd.DataFrame(index=membership.index, data=data)\n",
    "        .where(membership))\n",
    "\n",
    "def plot_desc(descs, **plot_pars):\n",
    "    cols = descs.columns\n",
    "    quantiles = list(cols[~cols.isin(['count', 'mean', 'std'])])\n",
    "    for cols in [\n",
    "        {'count'} & set(cols),\n",
    "        sorted({'mean', 'std'} & set(cols)),\n",
    "        quantiles]:\n",
    "        plt.figure()\n",
    "        descs.loc[:, list(cols)].plot(grid=True, **plot_pars)\n",
    "        plt.show()\n",
    "\n",
    "def plot_desc_pfo_matrix(weights, percentiles=None, **plot_pars):\n",
    "    descs = weights.T.describe(percentiles).T\n",
    "    plot_desc(descs, **plot_pars)\n",
    "\n",
    "def standard_scaler(weights):\n",
    "    means = weights.mean(axis=1)\n",
    "    stds = weights.std(axis=1)\n",
    "    return weights.sub(means, axis=0).div(stds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "IND = 150\n",
    "membership = gen_membership(5000, 0.5)\n",
    "\n",
    "weights = gen_data(membership)\n",
    "betas = gen_data(membership)\n",
    "plot_desc_pfo_matrix(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-packaging",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized = standard_scaler(weights)\n",
    "plot_desc_pfo_matrix(standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-bubble",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.iloc[IND].hist(bins=100, alpha=0.3)\n",
    "standardized.iloc[IND].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-cause",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.iloc[IND].hist(bins=100, alpha=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-complex",
   "metadata": {},
   "source": [
    "# Neutralize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-violation",
   "metadata": {},
   "source": [
    "## Utils and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proj_line(inner_prod, vec, unit_dir_vec):\n",
    "    return inner_prod(vec, unit_dir_vec) * unit_dir_vec\n",
    "\n",
    "def normalize(inner_prod, vec):\n",
    "    return vec * inner_prod(vec, vec) ** -.5\n",
    "\n",
    "def proj_orthonormal_basis(inner_prod, vec, orthon_basis):\n",
    "    return vec - sum([proj_line(inner_prod, vec, vec_dir) for vec_dir in orthon_basis])\n",
    "\n",
    "def canonical_orthonormalize(vecs):\n",
    "    basis = qr(np.array(vecs).T, mode='economic')[0]\n",
    "    return list(basis.T)\n",
    "\n",
    "def gram_schmidt_process(inner_prod, vecs):\n",
    "    append_one = lambda vecs_so_far, vec: vecs_so_far + [normalize(\n",
    "        inner_prod, proj_orthonormal_basis(inner_prod, vec, vecs_so_far))]\n",
    "    return reduce(append_one, vecs, [])\n",
    "\n",
    "def ortho_proj(inner_prod, vec, vecs):\n",
    "    process = (\n",
    "        canonical_orthonormalize if inner_prod.__name__ == 'dot' else\n",
    "        lambda x: gram_schmidt_process(inner_prod, x))\n",
    "    orthon_basis = process(vecs)\n",
    "#     orthon_basis = gram_schmidt_process(inner_prod, vecs)\n",
    "    return proj_orthonormal_basis(inner_prod, vec, orthon_basis)\n",
    "\n",
    "def gram_matrix(inner_prod, vecs):\n",
    "    return np.array([[inner_prod(vec_1, vec_2) for vec_1 in vecs] for vec_2 in vecs])\n",
    "\n",
    "def proj_hyperplane(inner_prod, vec, or_vec):\n",
    "    basis = [normalize(inner_prod, or_vec)]\n",
    "    return proj_orthonormal_basis(inner_prod, vec, basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-distinction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-bidding",
   "metadata": {},
   "outputs": [],
   "source": [
    "inn = np.dot\n",
    "vec = np.random.randn(10)\n",
    "unit_vec = normalize(inn, vec)\n",
    "np.linalg.norm(unit_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retired-minimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = proj_line(inn, vec, unit_vec)\n",
    "np.abs(inn(proj, unit_vec)), np.linalg.norm(proj) * np.linalg.norm(unit_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-jersey",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = [np.random.randn(10) for _ in range(3)]\n",
    "orthon_basis = canonical_orthonormalize(vecs)\n",
    "gram_matrix(inn, orthon_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = proj_orthonormal_basis(inn, vec, orthon_basis)\n",
    "[inn(proj, vec_) for vec_ in orthon_basis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accomplished-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "orthon_basis_2 = gram_schmidt_process(inn, vecs)\n",
    "gram_matrix(inn, orthon_basis_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-seafood",
   "metadata": {},
   "outputs": [],
   "source": [
    "[vec_1 - vec_2 for vec_1, vec_2 in zip(orthon_basis, orthon_basis_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-nightmare",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_2 = proj_orthonormal_basis(inn, vec, orthon_basis_2)\n",
    "[inn(proj_2, vec_) for vec_ in orthon_basis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "juvenile-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj - proj_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historic-wagner",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_3 = ortho_proj(inn, vec, vecs)\n",
    "proj_3 - proj_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personalized-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-parameter",
   "metadata": {},
   "outputs": [],
   "source": [
    "inn = lambda x, y: np.cov(x, y)[0][1]\n",
    "vec = np.random.randn(10)\n",
    "unit_vec = normalize(inn, vec)\n",
    "np.linalg.norm(unit_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-aircraft",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = proj_line(inn, vec, unit_vec)\n",
    "np.abs(inn(proj, unit_vec)), np.linalg.norm(proj) * np.linalg.norm(unit_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-metabolism",
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = [np.random.randn(10) for _ in range(3)]\n",
    "orthon_basis = canonical_orthonormalize(vecs)\n",
    "gram_matrix(inn, orthon_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = proj_orthonormal_basis(inn, vec, orthon_basis)\n",
    "[inn(proj, vec_) for vec_ in orthon_basis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selected-wright",
   "metadata": {},
   "outputs": [],
   "source": [
    "orthon_basis_2 = gram_schmidt_process(inn, vecs)\n",
    "gram_matrix(inn, orthon_basis_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "[vec_1 - vec_2 for vec_1, vec_2 in zip(orthon_basis, orthon_basis_2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-fishing",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_2 = proj_orthonormal_basis(inn, vec, orthon_basis_2)\n",
    "[inn(proj_2, vec_) for vec_ in orthon_basis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj - proj_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-alexander",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_3 = ortho_proj(inn, vec, vecs)\n",
    "proj_3 - proj_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-omaha",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-learning",
   "metadata": {},
   "outputs": [],
   "source": [
    "or_vec = np.random.randn(10)\n",
    "proj = proj_hyperplane(inn, vec, or_vec)\n",
    "inn(proj, or_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-orleans",
   "metadata": {},
   "source": [
    "## Neutralizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(inn, weights, betas, membership):\n",
    "    weights_t = weights.where(membership).fillna(0).T\n",
    "    betas_t = betas.where(membership).fillna(0).T\n",
    "    return (\n",
    "        weights_t.combine(betas_t, lambda x, y: proj_hyperplane(inn, x, y))\n",
    "        .T.where(membership))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blind-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "neutralized_standardized = neutralize(np.dot, standardized, betas, membership)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized.mul(betas).sum(axis=1).plot()\n",
    "neutralized_standardized.mul(betas).sum(axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_desc_pfo_matrix(neutralized_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized.iloc[IND].hist(bins=100, alpha=0.3)\n",
    "neutralized_standardized.iloc[IND].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-andrews",
   "metadata": {},
   "source": [
    "## Neutralizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize_multi(select_date, membership):\n",
    "    with Pool() as pool:\n",
    "        residuals = dict(pool.imap_unordered(select_date, membership.index))\n",
    "    return (\n",
    "        pd.DataFrame(residuals)\n",
    "        .T\n",
    "        .where(membership)\n",
    "        .sort_index()\n",
    "        )\n",
    "\n",
    "factors = [gen_data(membership) for _ in range(5)]\n",
    "kwargs = {\n",
    "    'weights_0': standardized.where(membership).fillna(0),\n",
    "    'factors_0': [factor.where(membership).fillna(0) for factor in factors],\n",
    "    'inn': lambda x, y: np.cov(x, y)[0][1],\n",
    "    # inn = lambda x, y: np.ma.cov(np.ma.masked_invalid(x), np.ma.masked_invalid(y))[0][1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def select_date(date):\n",
    "    wei = kwargs['weights_0'].loc[date, :]\n",
    "    facs = [factor.loc[date, :] for factor in kwargs['factors_0']]\n",
    "    return date, ortho_proj(kwargs['inn'], wei, facs)\n",
    "\n",
    "neutralized_standardized = neutralize_multi(select_date, membership)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supreme-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "corrs_neutralized_standardized = pd.concat([\n",
    "    neutralized_standardized.corrwith(factor, axis=1) for factor in factors],\n",
    "    axis=1)\n",
    "corrs_neutralized_standardized.plot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-procurement",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_desc_pfo_matrix(neutralized_standardized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized.iloc[IND].hist(bins=100, alpha=0.3)\n",
    "neutralized_standardized.iloc[IND].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-hygiene",
   "metadata": {},
   "source": [
    "# Clustering correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://gmarti.gitlab.io/qfin/2020/03/22/herc-part-i-implementation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-secretary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_corr(corr_df):\n",
    "    names = np.array(list(corr_df))\n",
    "    corr = corr_df.values\n",
    "    dissimilarities = 1 - corr\n",
    "    condensed = dissimilarities[np.triu_indices(len(corr_df), k=1)]\n",
    "    link = linkage(condensed, method='ward')\n",
    "    perm = leaves_list(optimal_leaf_ordering(link, condensed))\n",
    "    sorted_corr_df = pd.DataFrame(\n",
    "        index=names[perm], columns=names[perm], data=corr[perm, :][:, perm])\n",
    "    return link, perm, sorted_corr_df\n",
    "\n",
    "def cut_linkage(link, n_clusters):\n",
    "    c_inds = fcluster(link, n_clusters, criterion='maxclust')\n",
    "    return sorted(Counter(c_inds).items(), key=lambda x: x[0])\n",
    "\n",
    "def plot_clusters(sorted_corr_df, clusters_sizes):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.pcolormesh(sorted_corr_df)\n",
    "#     sns.heatmap(sorted_corr_df)  # import seaborn as sns\n",
    "    sizes = np.cumsum([0] + [y for _, y in clusters_sizes])\n",
    "    dim = len(sorted_corr_df)\n",
    "    for left, right in zip(sizes, sizes[1:]):\n",
    "        plt.axvline(x=left, ymin=left / dim, ymax=right / dim, color='r')\n",
    "        plt.axvline(x=right, ymin=left / dim, ymax=right / dim, color='r')\n",
    "        plt.axhline(y=left, xmin=left / dim, xmax=right / dim, color='r')\n",
    "        plt.axhline(y=right, xmin=left / dim, xmax=right / dim, color='r')\n",
    "    cols = iter(list(sorted_corr_df))\n",
    "    print([list(islice(cols, n_eles)) for _, n_eles in clusters_sizes])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acb52b3",
   "metadata": {},
   "source": [
    "# chi2 independence test for categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac15f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_val_chi2_categ_indep(categs, col_0, col_1):\n",
    "    p_val = chi2_contingency(pd.crosstab(\n",
    "        categs[col_0], categs[col_1]))[1]\n",
    "    return [[col_0, col_1, p_val], [col_1, col_0, p_val]]\n",
    "\n",
    "def p_vals_chi2_categs_indep(categs):\n",
    "    lst = [\n",
    "        p_val_chi2_categ_indep(categs, col_0, col_1)\n",
    "        for col_0, col_1 in combinations(categs.columns, r=2)\n",
    "    ]\n",
    "    p_vals = (\n",
    "        pd.DataFrame([ele for sub in lst for ele in sub])\n",
    "        .pivot(index=0, columns=1, values=2)\n",
    "        .fillna(0))\n",
    "    p_vals.index.name = None\n",
    "    p_vals.columns.name = None\n",
    "    return p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcddafb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_str = 'abcdefghijklmnopqrstuvwxyz'\n",
    "splits = [base_str[3 * n : 3 * n + 3] for n in range(len(base_str) // 3)]\n",
    "categs = pd.DataFrame([\n",
    "    np.random.choice(a=list(chars), size=100)\n",
    "    for chars in splits]).T\n",
    "p_vals = p_vals_chi2_categs_indep(categs)\n",
    "p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6972936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "link, perm, sorted_p_vals = sort_corr(1 - p_vals)\n",
    "sizes = cut_linkage(link, n_clusters)\n",
    "plot_clusters(sorted_p_vals, sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e2fe1f-1691-4442-ad89-37a0d11be0e9",
   "metadata": {},
   "source": [
    "# Rolling win_type"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0aa1a8b-aab6-4fdc-9ff6-e37e3aad1be5",
   "metadata": {},
   "source": [
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.windows.exponential.html#scipy.signal.windows.exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab02ba-7738-42ab-b1fa-e5d49f921087",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 10\n",
    "rands = pd.Series(np.random.randn(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af40b26c-750b-431d-b5ec-7466d7d33ca6",
   "metadata": {},
   "source": [
    "## Exponential rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2144695b-a406-4921-a7fd-e5a587adefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 3\n",
    "\n",
    "r_pandas = (\n",
    "    rands\n",
    "    .rolling(window, win_type='exponential')\n",
    "    .mean(tau=tau, center=0, sym=False)\n",
    ")\n",
    "rands.plot()\n",
    "r_pandas.plot(grid=True)\n",
    "\n",
    "xxx = np.arange(window)\n",
    "weights = np.exp(-xxx / tau)\n",
    "integral = weights.sum() / len(weights)\n",
    "weights /= integral\n",
    "plt.plot(xxx, weights)\n",
    "weights.sum() / len(weights)\n",
    "\n",
    "r_manual = rands.rolling(window).apply(lambda x: (weights * x).mean())\n",
    "norm((r_pandas - r_manual).iloc[window - 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e0636b-8474-4cce-be1a-e6388954afd8",
   "metadata": {},
   "source": [
    "## Rolling mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ced00e-f98d-45cc-afa3-2c73e9a10988",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_pandas = (\n",
    "    rands\n",
    "    .rolling(window)\n",
    "    .mean()\n",
    ")\n",
    "rands.plot()\n",
    "r_pandas.plot(grid=True)\n",
    "\n",
    "xxx = np.arange(window)\n",
    "weights = np.ones(len(xxx))\n",
    "integral = weights.sum() / len(weights)\n",
    "weights /= integral\n",
    "plt.plot(xxx, weights)\n",
    "weights.sum() / len(weights)\n",
    "\n",
    "r_manual = rands.rolling(window).apply(lambda x: (weights * x).mean())\n",
    "norm((r_pandas - r_manual).iloc[window - 1:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
